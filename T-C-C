# -*- codeing = utf-8 -*-
# @Time:2021/10/6 8:03
# @Author:A20190277
# @File:T-C-C.py
# @Software:PyCharm

''''''
'''建立模型，根据模型的建立找出客户流失与哪些因素有关，并且提出建议，降低用户流失率'''
#对数据先浏览后进行导入数据
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from collections import Counter
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus']=False


df1=pd.read_csv(r"C:\Users\18356\Desktop\Kaggle\Telco Customer Churn\WA_Fn-UseC_-Telco-Customer-Churn.csv")
# print(df1.info())           #不存在缺失值，有数据的数据类型错误，进行修改
# print(df1.describe())

#先将数据类型修改正确
df1['TotalCharges']=pd.to_numeric(df1['TotalCharges'], errors='coerce')
#print(df1.info())            #发现totalcharges存在缺失值，但由于数据量较小，直接删除即可

#删除缺失值
mis_val_percent = 100 * df1.isnull().sum() / len(df1) # 缺失值比例
#print(mis_val_percent)                #缺失值占比很小，只占0.15%，所以进行删除处理
# mis=100*df1['TotalCharges'].isnull().sum()/len(df1)
# print(mis)
df2=df1[df1['TotalCharges'].notnull()]
df2=df2.reset_index()[df2.columns]
# print(df2.info())
# print(df2.describe())

#异常值检测---该数据集中只需对费用列进行异常值检测
#df2['TotalCharges'].plot.box
#plt.boxplot(df2['TotalCharges'])
#plt.boxplot(df2['MonthlyCharges'])
#plt.show()             #无异常值

#EDA：对数据进行可视化，对数据进行进一步了解，对后续变量的筛选奠定基础

#对Churn--因变量进行可视化分析（饼图）
#plt.rcParams['figure.figsize']=6,6
#plt.pie(df2['Churn'].value_counts(),labels=df2['Churn'].value_counts().index,autopct='%1.2f%%',explode=(0.1,0))  #保留两位小数，分离饼图
#plt.title('Churn(Yes/No)饼图')
#plt.savefig('Churn(Yes or No)饼图',dpi=600)
#plt.show()
print('可看出该数据为属于不平衡数据集，流失用户占比达26.58%，在后续建模种对不平衡数据需要进行处理（SMOTE）')

#老年人与流失之间的关系（1：是老人，0：不是老人）---累计柱状图
Sen_a=df2['SeniorCitizen'][df2['Churn']=='Yes'].value_counts()/len(df2)
Sen_b=df2['SeniorCitizen'][df2['Churn']=='No'].value_counts()/len(df2)

SenDf=pd.DataFrame({'Yes':Sen_a,'No':Sen_b})
# SenDf.plot.bar(stacked='True')
def to_percent(temp, position):
    return '%1.0f'%(100*temp) + '%'
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('老人与客户流失之间的关系')
# plt.savefig('老人与客户流失之间的关系',dpi=600)
# plt.show()
print('可看出当研究对象为老人时，No在老人占比较多，所以说明老人相比较年轻人来说流失严重（差别有些大）')

#性别与流失之间的关系---累计柱状图
Gen_a=df2['gender'][df2['Churn']=='Yes'].value_counts()/len(df2)
Gen_b=df2['gender'][df2['Churn']=='No'].value_counts()/len(df2)
GenDf=pd.DataFrame({'Yes':Gen_a,'No':Gen_b})
# GenDf.plot.bar(stacked='True')
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('性别与客户流失之间的关系')
# plt.savefig('性别与客户流失之间的关系',dpi=600)
# plt.show()
print('可看出性别与流失无关，所以后续建模时可剔除该变量')

#有无配偶与流失之间的关系---累计柱状图
Par_a=df2['Partner'][df2['Churn']=='Yes'].value_counts()/len(df2)
Par_b=df2['Partner'][df2['Churn']=='No'].value_counts()/len(df2)
ParDf=pd.DataFrame({'C_Yes':Par_a,'C_No':Par_b})
# ParDf.plot.bar(stacked='True')
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('配偶和客户流失之间的关系')
# plt.xticks([0,1],['无配偶','有配偶'],rotation=1)
# plt.savefig('配偶和客户流失之间的关系',dpi=600)
# plt.show()
print('可以看出无配偶的客户流失率较高（差别不是很大）')

#是否独居与流失之间的关系---累计柱状图
Dep_a=df2['Dependents'][df2['Churn']=='Yes'].value_counts()/len(df2)
Dep_b=df2['Dependents'][df2['Churn']=='No'].value_counts()/len(df2)
DepDf=pd.DataFrame({'C_Yes':Dep_a,'C_No':Dep_b})
# DepDf.plot.bar(stacked='True')
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('独居和客户流失之间的关系')
# plt.xticks([0,1],['不独居','独居'],rotation=1)
# plt.savefig('独居和客户流失之间的关系',dpi=600)
# plt.show()
print('可以看出不独居的客户流失率较高（差别较大）')

#合同时长与流失之间的关系---累计柱状图
Con_a=df2['Contract'][df2['Churn']=='Yes'].value_counts()/len(df2)
Con_b=df2['Contract'][df2['Churn']=='No'].value_counts()/len(df2)
ConDf=pd.DataFrame({'C_Yes':Con_a,'C_No':Con_b})
# ConDf.plot.bar(stacked='True')
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('合同时长和客户流失之间的关系')
# plt.xticks([0,1,2],['Month-to-month','One year','Two year'],rotation=1)
# plt.savefig('合同时长和客户流失之间的关系',dpi=600)
# plt.show()
print('可以明显看出，合同时长越长，流失比例越小')

#是否有电子账单与流失之间的关系---累计柱状图
Pap_a=df2['PaperlessBilling'][df2['Churn']=='Yes'].value_counts()/len(df2)
Pap_b=df2['PaperlessBilling'][df2['Churn']=='No'].value_counts()/len(df2)
PapDf=pd.DataFrame({'C_Yes':Pap_a,'C_No':Pap_b})
# PapDf.plot.bar(stacked='True')
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('电子账单和客户流失之间的关系')
# plt.xticks([0,1],['有电子账单','无电子账单'],rotation=1)
# plt.savefig('电子账单和客户流失之间的关系',dpi=600)
# plt.show()
print('可以看出无电子账单的流失率稍低于另一种')

#支付方式与流失之间的关系---累计柱状图
Pay_a=df2['PaymentMethod'][df2['Churn']=='Yes'].value_counts()/len(df2)
Pay_b=df2['PaymentMethod'][df2['Churn']=='No'].value_counts()/len(df2)
PayDf=pd.DataFrame({'C_Yes':Pay_a,'C_No':Pay_b})
# PayDf.plot.bar(stacked='True')
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('支付方式和客户流失之间的关系')
# plt.xticks([0,1,2,3],['Bank transfer (automatic)','Credit card (automatic)','Electronic check','Mailed check'],rotation=1)
# plt.savefig('支付方式和客户流失之间的关系',dpi=600)
# plt.show()
print('可以看出选择Electronic check支付方式的流失比例较大，该支付方式需要改进')


#由于下面数据量过于复杂，先对数据内容进行简单的查看
# obj_var=df2.select_dtypes(['object'])       #提取数据类型为object的数据
# def unique_value(col):
#     print(col,':',df2[col].unique())
# for i in range(1,len(obj_var.columns)):
#     unique_value(obj_var.columns[i])

#电话服务和流失之间的关系---累计柱状图
Pho_a=df2['PhoneService'][df2['Churn']=='Yes'].value_counts()/len(df2)
Pho_b=df2['PhoneService'][df2['Churn']=='No'].value_counts()/len(df2)
PhoDf=pd.DataFrame({'C_Yes':Pho_a,'C_No':Pho_b})
# PhoDf.plot.bar(stacked='True')
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('电话服务和客户流失之间的关系')
# plt.xticks([0,1],['无电话服务','有电话服务'],rotation=1)
# plt.savefig('电话服务和客户流失之间的关系',dpi=600)
# plt.show()
print('可以看出是否采用电话服务对客户流失率无明显影响（无论是否采用电话服务，客户流失比例是相似的），后续建模剔除该变量')

#网络服务和流失之间的关系---累计柱状图
Int_a=df2['InternetService'][df2['Churn']=='Yes'].value_counts()/len(df2)
Int_b=df2['InternetService'][df2['Churn']=='No'].value_counts()/len(df2)
IntDf=pd.DataFrame({'C_Yes':Int_a,'C_No':Int_b})
# IntDf.plot.bar(stacked='True')
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('网络服务和客户流失之间的关系')
# plt.xticks([0,1,2],['DSL','Fiber optic','No'],rotation=1)
# plt.savefig('网络和客户流失之间的关系',dpi=600)
# plt.show()
print('可以看出，不同的网络服务对客户流失有很大的不同，其中Fiber optic服务可以较差，使得近一半的用户流失')

#多条电话服务线路和流失之间的关系---累计柱状图
Mul_a=df2['MultipleLines'][df2['Churn']=='Yes'].value_counts()/len(df2)
Mul_b=df2['MultipleLines'][df2['Churn']=='No'].value_counts()/len(df2)
MulDf=pd.DataFrame({'C_Yes':Mul_a,'C_No':Mul_b})
# MulDf.plot.bar(stacked='True')
# plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))
# plt.title('多条电话服务线路和客户流失之间的关系')
# plt.xticks([0,1,2],['No','No phone service','Yes'],rotation=1)
# plt.savefig('多条电话服务线路和客户流失之间的关系',dpi=600)
# plt.show()
print('可以看出，选择多条电话服务线路的客户流失率比例较低')

#将plot以窗口显示：Setting-Tools-Diff&Merge-Python Scientic 去掉对勾

#关于网络服务的后续附加值，一起进行讨论OnlineSecurity ，OnlineBackup ，DeviceProtection ，TechSupport ，StreamingTV ，StreamingMovies
#在客户选择了网络服务的前提下进行
df3=df2[df2['InternetService']!='No']
# df3['InternetServices_Count'] = (df3[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']] == 'Yes').sum(axis=1)
# plt.figure(figsize=(12,6))
# ax = sns.countplot(x='InternetServices_Count', hue='Churn', data=df3, palette=("Set3"))#统计x中每个类别的数量
# ax.set_title('网络额外服务量与客户流失之间的关系', fontsize=16)
# plt.savefig('网络额外服务量与客户流失之间的关系', dpi=600)
# plt.show()
print('可以看出，客户选择的额外服务越多，那么流失率就会越少')

#入网时间与客户流失之间的关系---核密度图像
#每月费用与客户流失之间的关系---核密度图像
#总费用与客户流失之间的关系---核密度图像
def kdeplot(feature):
    plt.figure(figsize=(9,4))
    ax0=sns.kdeplot(df2[df2['Churn']=='No'][feature],color='navy',label='C_No',shade='True')
    ax1=sns.kdeplot(df2[df2['Churn']=='Yes'][feature],color='orange',label='C_Yes',shade='True')
    # plt.legend(fontsize=10)
    # plt.title("{0}与客户流失之间的关系".format(feature))
    # plt.savefig('{0}与客户流失之间的关系'.format(feature), dpi=600)
# kdeplot('tenure')
# kdeplot('MonthlyCharges')
# kdeplot('TotalCharges')
# plt.show()
print('根据核密度图像可得出这三种因素分别在哪些区间内客户流失率较低，关于总费用的猜想：总费用与月费用和入网时间有较大关系（corr函数查看一下）')

#总费用与月费用和入网时间相关性的查看
# plt.figure(figsize=(15, 10))
# sns.heatmap(df2[['tenure','MonthlyCharges','TotalCharges']].corr(),annot=True, vmax=1, square=True)  #矩阵热力图
# plt.xticks(fontsize=12)
# plt.yticks(fontsize=12)
# plt.savefig('总费用与月费用和入网时间相关性的查看', dpi=600)
# plt.show()
print(df2[['tenure','MonthlyCharges','TotalCharges']].corr())
print('可以看出，总费用与月费用和入网时间有较强的相关性，猜想：后续建模可以降维')

print('------------------数据可视化结束，开始建模！---------------------------------')
print('首先处理数据！！！')
#先剔除数据：ID，gender，PhoneService这三个根据数据可视化得到的与流失无关的数据
customerID=df2['customerID']
df2.drop(['customerID','gender','PhoneService'],axis=1,inplace=True)
# print(df2.info())
#再对数据进行one-hot编码
#对因变量进行处理map映射
Churn_mapDict={'Yes':1,'No':0}
df2['Churn']=df2['Churn'].map(Churn_mapDict)
#print(df2.info())
#对自变量进行处理，为了避免数据冗余，将No internet service记为No
obj_var=df2.select_dtypes(['object'])
def unique_value(col):
    print(col,':',df2[col].unique())
for i in range(1,len(obj_var.columns)):
    unique_value(obj_var.columns[i])
#1
PartnerDf=pd.get_dummies(df2['Partner'],prefix='Partner')
df2=pd.concat([df2,PartnerDf],axis=1)
df2.drop('Partner',axis=1,inplace=True)
#2
DependentsDf=pd.get_dummies(df2['Dependents'],prefix='Dependents')
df2=pd.concat([df2,DependentsDf],axis=1)
df2.drop('Dependents',axis=1,inplace=True)
#3
df2['MultipleLines'].replace('No phone service','No',inplace=True)
MultipleLinesDf=pd.get_dummies(df2['MultipleLines'],prefix='MultipleLines')
df2=pd.concat([df2,MultipleLinesDf],axis=1)
df2.drop('MultipleLines',axis=1,inplace=True)
#4
InternetServiceDf=pd.get_dummies(df2['InternetService'],prefix='InternetService')
df2=pd.concat([df2,InternetServiceDf],axis=1)
df2.drop('InternetService',axis=1,inplace=True)
'''
#5
df2.replace('No internet service','No',inplace=True)
OnlineSecurityDf=pd.get_dummies(df2['OnlineSecurity'],prefix='OnlineSecurity')
df2=pd.concat([df2,OnlineSecurityDf],axis=1)
df2.drop('OnlineSecurity',axis=1,inplace=True)
#6
OnlineBackupDf=pd.get_dummies(df2['OnlineBackup'],prefix='OnlineBackup')
df2=pd.concat([df2,OnlineBackupDf],axis=1)
df2.drop('OnlineBackup',axis=1,inplace=True)
#7
DeviceProtectionDf=pd.get_dummies(df2['DeviceProtection'],prefix='DeviceProtection')
df2=pd.concat([df2,DeviceProtectionDf],axis=1)
df2.drop('DeviceProtection',axis=1,inplace=True)
#8
TechSupportDf=pd.get_dummies(df2['TechSupport'],prefix='TechSupport')
df2=pd.concat([df2,TechSupportDf],axis=1)
df2.drop('TechSupport',axis=1,inplace=True)
#9
StreamingTVDf=pd.get_dummies(df2['StreamingTV'],prefix='StreamingTV')
df2=pd.concat([df2,StreamingTVDf],axis=1)
df2.drop('StreamingTV',axis=1,inplace=True)
#10
StreamingMoviesDf=pd.get_dummies(df2['StreamingMovies'],prefix='StreamingMovies')
df2=pd.concat([df2,StreamingMoviesDf],axis=1)
df2.drop('StreamingMovies',axis=1,inplace=True)
'''
#11
ContractDf=pd.get_dummies(df2['Contract'],prefix='Contract')
df2=pd.concat([df2,ContractDf],axis=1)
df2.drop('Contract',axis=1,inplace=True)
#12
PaperlessBillingDf=pd.get_dummies(df2['PaperlessBilling'],prefix='PaperlessBilling')
df2=pd.concat([df2,PaperlessBillingDf],axis=1)
df2.drop('PaperlessBilling',axis=1,inplace=True)
#13
PaymentMethodDf=pd.get_dummies(df2['PaymentMethod'],prefix='PaymentMethod')
df2=pd.concat([df2,PaymentMethodDf],axis=1)
df2.drop('PaymentMethod',axis=1,inplace=True)
print('----------------------------------------------------')
#print(df2.info())
print('--------------对数据进行相关性分析，并且绘图------------')
# plt.figure(1)
# sns.heatmap(df2.corr(),annot=True,fmt='.2f', annot_kws={'size':3},vmax=1,cmap="YlGnBu", square=True)
# plt.xticks(fontsize=9)
# plt.yticks(fontsize=9)
# plt.title('数据相关性分析图')
# plt.savefig('数据相关性分析热力图',dpi=300,bbox_inches='tight')
# plt.show()
# plt.figure(figsize=(16,8))
# df2.corr()['Churn'].sort_values(ascending=False).plot(kind='bar')
# plt.title('数据相关性分析图')
# plt.savefig('数据相关性分析条形图',dpi=300,bbox_inches='tight')
# plt.show()

#df3=df2[df2['InternetService']!='No']
df2['InternetServices_Count'] = (df2[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']] == 'Yes').sum(axis=1)
df2.drop(['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies'],axis=1,inplace=True)
#print(df2.info())
#print(df2['InternetServices_Count'].head(20))

print('对连续数据离散化处理---等距分箱后one-hot编码处理')

def tenure_category(tenure):
    if tenure <=12:
        return '0-12'
    elif (tenure>12) & (tenure<=24):
        return '12-24'
    elif (tenure>24) & (tenure<=36):
        return '24-36'
    elif (tenure>36) & (tenure<=48):
        return '36-48'
    elif (tenure>48) & (tenure<=60):
        return '48-60'
    else:
        return 'above_60'

df2['tenure_category'] = df2['tenure'].map(tenure_category)

tenure_categoryDf=pd.get_dummies(df2['tenure_category'],prefix='tenure_category')
df2=pd.concat([df2,tenure_categoryDf],axis=1)
df2.drop('tenure_category',axis=1,inplace=True)

print('---------------数据标准化--------------')
from sklearn.preprocessing import StandardScaler      #数据标准化

df4=df2.copy()
num_var = ['MonthlyCharges','TotalCharges','InternetServices_Count']
scaler = StandardScaler()
df4[num_var] = scaler.fit_transform(df4[num_var])
# print(df4.info())
#print(df4.MonthlyCharges.head(10),df4.InternetServices_Count.head(10))
df5=df4[['MonthlyCharges','TotalCharges','tenure_category_0-12','tenure_category_12-24','tenure_category_24-36','tenure_category_36-48','tenure_category_48-60','tenure_category_above_60']]
# sns.heatmap(df5.corr(),annot=True,fmt='.2f', annot_kws={'size':7},vmax=1,cmap="YlGnBu", square=True)
# plt.title('总费用与入网时长和月费用相关性分析图')
# plt.savefig('总费用与入网时长和月费用相关性分析热力图',dpi=300,bbox_inches='tight')
# plt.show()
print('可以看出相关性还是较高，所以可以降维处理')
#print(df4.info())
df4.drop(['tenure','TotalCharges'],axis=1,inplace=True)
#print(df4.info())

print('数据处理完毕！筛选特征值完毕！开始建模！！！！！！')

print('先划分训练集和测试集，后对训练集的数据进行SMOTE处理')
from sklearn.model_selection import train_test_split            #划分训练集和测试集
import imblearn
from imblearn.over_sampling import SMOTE

columns = df4.columns.tolist()
columns.remove('Churn')
print(columns ,type(columns))
features = df4[columns].values
target = df4['Churn'].values
# stratify = target 表示按标识的类别，作为训练数据集、测试数据集内部的分配比例
#参数stratify： 依据标签y，按原数据y中各类比例，分配给train和test，使得train和test中各类数据的比例与原数据集一样。
train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.30, stratify = target, random_state = 50)
print('训练集（自变量）:',train_x.shape,'：4922条数据，共27个自变量')    #数据类型是'numpy.ndarray'了
print('----------------------------------------------')
print('训练集（因变量）:',train_y.shape,'：4922条数据')
print(Counter(train_y))                                             #Churn_mapDict={'Yes':1,'No':0} ：1--流失，0--不流失
print('----------------------------------------------')
print('测试集（自变量）:',test_x.shape,'：2110条数据，共27个自变量')
print('----------------------------------------------')
print('测试集（因变量）:',test_y.shape,'：2110条数据')
print(Counter(test_y))

print('数据集划分完毕！接下来对训练集进行SMOTE处理！')

smt = SMOTE(random_state = 50)
smt_X,smt_Y = smt.fit_resample(train_x,train_y)
print('SMOTE后的训练集（自变量）:',smt_X.shape,'：7228条数据，共27个自变量')
print('----------------------------------------------')
print('SMOTE后的训练集（因变量）:',smt_Y.shape,'：7228条数据')
print('----------------------------------------------')
print(Counter(smt_Y))             #可看出数据均衡了
smt_X = pd.DataFrame(data = smt_X,columns=columns)
smt_Y = pd.DataFrame(data = smt_Y,columns=['Churn'])

print('利用训练集数据进行模型建立！')
print('建立多个模型，对比模型优劣，找出最优模型！')
print('Logistic、RandomForest进行对比')
from sklearn.linear_model import LogisticRegression             #Logistic回归模型
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
from sklearn.metrics import roc_auc_score,roc_curve             #ROC曲线
from sklearn.metrics import f1_score                            #F1 score是精确率和召回率的调和平均值，f1越高，精度越高：https://blog.csdn.net/tttwister/article/details/81138865?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163385217616780265479171%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163385217616780265479171&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-81138865.first_rank_v2_pc_rank_v29&utm_term=from+sklearn.metrics+import+precision_score%2Crecall_score+&spm=1018.2226.3001.4187
import statsmodels.api as sm                                    #
from sklearn.metrics import precision_score,recall_score        #
from yellowbrick.classifier import DiscriminationThreshold      #

print('Logistic:')
def churn_prediction(algorithm, X_training, X_testing, y_training, y_testing):
    algorithm.fit(X_training, y_training)
    predictions = algorithm.predict(X_testing)          #predict返回的是一个预测的值
    probabilities = algorithm.predict_proba(X_testing)  #predict_proba返回的是对于预测为各个类别的概率
    print(algorithm)
    print("\n Classification report : \n", classification_report(y_testing, predictions))   #输出，精度、召回率、F1和每个标签出现的次数：https://blog.csdn.net/akadiao/article/details/78788864?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163385370616780357272336%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163385370616780357272336&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-78788864.first_rank_v2_pc_rank_v29&utm_term=classification_report&spm=1018.2226.3001.4187
    print("Accuracy   Score : ", accuracy_score(y_testing, predictions))                    #分类准确率：分类正确率分数
    print("Recall Score :", recall_score(y_testing, predictions))                           #召回率：就是正例被分为正例的比例
    print("F1 Score :", f1_score(y_testing, predictions))                                   #f1_score是accuracy_score和recall_score的加权数
    # roc_auc_score
    model_roc_auc = roc_auc_score(y_testing, predictions)                                   #返回曲线下面积
    print("Area under curve : ", model_roc_auc, "\n")
    fpr, tpr, thresholds = roc_curve(y_testing, probabilities[:, 1])
    # confusion matrix
    conf_matrix = confusion_matrix(y_testing, predictions)                                  #混淆矩阵
    print("Confusion Matrix : \n", conf_matrix)


LR_smote = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True,
                   intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear',
                   max_iter=100, multi_class='ovr', verbose=0, warm_start=False,
                   n_jobs=None, l1_ratio=None)
#LogisticRegression参数详解
#https://blog.csdn.net/duyibo123/article/details/110044873?ops_request_misc=&request_id=&biz_id=102&utm_term=LogisticRegression%E5%8F%82%E6%95%B0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-110044873.nonecase&spm=1018.2226.3001.4187
churn_prediction(LR_smote,smt_X, test_x,smt_Y,test_y)

print('--------------------------------------------------------------------------------------')

print('RandomForest')

from sklearn.ensemble import RandomForestClassifier
plt.figure(figsize=(12,7))
rf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None,
        min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
        max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,
        bootstrap=True, oob_score=False, n_jobs=None,
        random_state=None, verbose=0, warm_start=False, class_weight=None)
#RandomForestClassifier参数
#https://blog.csdn.net/qq_42479987/article/details/109549166?ops_request_misc=&request_id=&biz_id=102&utm_term=%20RandomForestClassifier%E5%8F%82%E6%95%B0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-5-109549166.nonecase&spm=1018.2226.3001.4187churn_prediction(rf,smt_X,test_x,smt_Y,test_y)
churn_prediction(rf,smt_X,test_x,smt_Y,test_y)
print('--------------------------------------------------------------------------------------')
print('随机森林寻找最优参数')
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.pipeline import Pipeline
# pipe = Pipeline([('classifier', RandomForestClassifier())])
# search_space = [{'classifier': [LogisticRegression()],
#                  'classifier__penalty': ['l1', 'l2'],
#                  'classifier__C': np.logspace(0, 4, 10)},
#                 {'classifier': [RandomForestClassifier()],
#                  'classifier__n_estimators': [10, 100, 1000],
#                  'classifier__max_features': [1, 2, 3]}]
#
# clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)
# best_model = clf.fit(smt_X, smt_Y)
#
# best_model.best_estimator_.get_params()['classifier']
# best_model.predict(test_x)
# clf_best_model = best_model
# churn_prediction(clf_best_model,smt_X,test_x,smt_Y,test_y)

#可看：https://github.com/baopuzi/Telco_Customer_Churn/blob/master/.ipynb_checkpoints/%E7%94%B5%E4%BF%A1%E5%AE%A2%E6%88%B7%E6%B5%81%E5%A4%B1%E5%88%86%E6%9E%90%E5%92%8C%E9%A2%84%E6%B5%8B-checkpoint.ipynb

model = LR_smote
model.fit(train_x,train_y)

# 提取customerID
print(df2.info())
pred_id = customerID.tail(10)
# 提取预测数据集特征
pred_x = df4.drop(['Churn'],axis=1).tail(10)

# 预测值
pred_y = model.predict(pred_x)

# 预测结果
predDf = pd.DataFrame({'customerID':pred_id, 'Churn':pred_y})
print(predDf)



